#! /bin/sh -e
## 28_cobalt_ide.dpatch by Peter Horton

## DP: Make IDE work faster on Cobalt...
## DP: Upstream status: Peter will check if this is still the preferred
## solution

## Without this patch it takes a Cobalt in 64bit mode over 2 minute to
## probe for IDE disks.


if [ $# -lt 1 ]; then
    echo "`basename $0`: script expects -patch|-unpatch as argument" >&2
    exit 1
fi

[ -f debian/patches/00patch-opts ] && . debian/patches/00patch-opts
patch_opts="${patch_opts:--f --no-backup-if-mismatch} ${2:+-d $2}"

case "$1" in
    -patch) patch -p1 ${patch_opts} < $0;;
    -unpatch) patch -R -p1 ${patch_opts} < $0;;
    *)
        echo "`basename $0`: script expects -patch|-unpatch as argument" >&2
        exit 1;;
esac

exit 0

--- /dev/null	2005-11-10 20:12:48.000000000 +0000
+++ b/include/asm-mips/cobalt/ide.h	2006-01-20 16:06:11.000000000 +0000
@@ -0,0 +1,83 @@
+
+/*
+ * PIO "in" transfers can cause D-cache lines to be allocated
+ * to the data being read. If the target is the page cache then
+ * the kernel can create a user space mapping of the same page
+ * without flushing it from the D-cache. This has large potential
+ * to create cache aliases. The Cobalts seem to trigger this
+ * problem easily.
+ *
+ * MIPS doesn't have a flush_dcache_range() so we roll
+ * our own.
+ *
+ * -- pdh
+ */
+
+#define MAX_HWIFS			2
+
+#include <asm/r4kcache.h>
+
+static inline void __flush_dcache(void)
+{
+	unsigned long dc_size, dc_line, addr, end;
+
+	dc_size = current_cpu_data.dcache.ways << current_cpu_data.dcache.waybit;
+	dc_line = current_cpu_data.dcache.linesz;
+
+	addr = CKSEG0;
+	end = addr + dc_size;
+
+	for (; addr < end; addr += dc_line)
+		flush_dcache_line_indexed(addr);
+}
+
+static inline void __flush_dcache_range(unsigned long start, unsigned long end)
+{
+	unsigned long dc_size, dc_line, addr;
+
+	dc_size = current_cpu_data.dcache.ways << current_cpu_data.dcache.waybit;
+	dc_line = current_cpu_data.dcache.linesz;
+
+	addr = start & ~(dc_line - 1);
+	end += dc_line - 1;
+
+	if (end - addr < dc_size)
+		for (; addr < end; addr += dc_line)
+			flush_dcache_line(addr);
+	else
+		__flush_dcache();
+}
+
+static inline void __ide_insw(unsigned long port, void *addr, unsigned int count)
+{
+	insw(port, addr, count);
+
+	__flush_dcache_range((unsigned long) addr, (unsigned long) addr + count * 2);
+}
+
+static inline void __ide_insl(unsigned long port, void *addr, unsigned int count)
+{
+	insl(port, addr, count);
+
+	__flush_dcache_range((unsigned long) addr, (unsigned long) addr + count * 4);
+}
+
+static inline void __ide_mm_insw(volatile void __iomem *port, void *addr, unsigned int count)
+{
+	readsw(port, addr, count);
+
+	__flush_dcache_range((unsigned long) addr, (unsigned long) addr + count * 2);
+}
+
+static inline void __ide_mm_insl(volatile void __iomem *port, void *addr, unsigned int count)
+{
+	readsl(port, addr, count);
+
+	__flush_dcache_range((unsigned long) addr, (unsigned long) addr + count * 4);
+}
+
+#define insw			__ide_insw
+#define insl			__ide_insl
+
+#define __ide_mm_outsw		writesw
+#define __ide_mm_outsl		writesl

